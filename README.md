ETL Pipeline with AWS (S3 & MySQL RDS)
ğŸ“Œ Project Overview
This project implements an ETL (Extract, Transform, Load) pipeline using Python, AWS S3, and MySQL RDS. The pipeline extracts data from CSV, JSON, and XML files, transforms it, and loads it into an AWS RDS MySQL database.

ğŸ› ï¸ Technologies Used
Python (Pandas, SQLAlchemy, Boto3, dotenv)
AWS S3 (Storage for raw and transformed data)
AWS RDS (MySQL) (Stores transformed data)
SQLAlchemy (Database interaction)
Logging (Tracks ETL execution)
ğŸ“ Project Directory Structure
graphql

ETL_Project/
â”‚â”€â”€ config/
â”‚   â”œâ”€â”€ .env                 # Stores AWS & DB credentials
â”‚   â”œâ”€â”€ aws_config.py        # Load AWS credentials
â”‚   â”œâ”€â”€ db_config.py         # Load RDS credentials
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ unzipped/            # Raw data extracted from source.zip
â”‚   â”œâ”€â”€ transformed/         # Processed CSV, JSON, XML files
â”‚â”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py           # Extract raw data from S3 or local
â”‚   â”œâ”€â”€ transform.py         # Transform data (cleaning, unit conversions)
â”‚   â”œâ”€â”€ load.py              # Load data to S3 and RDS
â”‚â”€â”€ logs/
â”‚   â”œâ”€â”€ etl.log              # Stores ETL execution logs
â”‚â”€â”€ main.py                  # ETL pipeline execution script
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ README.md                # Project documentation
ğŸš€ Features
âœ… Extracts data from CSV, JSON, and XML files
âœ… Downloads/uploads data from AWS S3
âœ… Transforms data (unit conversion, cleaning)
âœ… Loads data into AWS MySQL RDS
âœ… Uses environment variables for secure credentials
âœ… Implements logging for tracking ETL execution

ğŸ”§ Installation & Setup
1ï¸âƒ£ Clone the Repository


git clone https://github.com/your-username/etl-pipeline-aws.git
cd etl-pipeline-aws
2ï¸âƒ£ Create & Activate Virtual Environment

python -m venv .venv
source .venv/bin/activate   # On macOS/Linux
.v.venv\Scripts\activate    # On Windows
3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt
4ï¸âƒ£ Configure AWS & MySQL Credentials
Create a .env file inside the config/ folder:

# AWS Credentials
AWS_ACCESS_KEY_ID=your-aws-access-key
AWS_SECRET_ACCESS_KEY=your-aws-secret-key
AWS_REGION=your-aws-region
S3_BUCKET_NAME=your-s3-bucket-name

# MySQL RDS Credentials
RDS_HOST=your-rds-endpoint
RDS_PORT=3306
RDS_USER=your-db-username
RDS_PASSWORD=your-db-password
RDS_DB=your-database-name
5ï¸âƒ£ Run the ETL Pipeline

python main.py
ğŸ“Š Database Schema (MySQL RDS)

CREATE TABLE IF NOT EXISTS transformed_table (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(255),
    height FLOAT,
    weight FLOAT
);
âš¡ AWS Setup Guide
ğŸ”¹ Create an S3 Bucket
Open AWS Console â†’ S3
Click Create Bucket â†’ Name it (your-s3-bucket-name)
Enable public access settings if needed
ğŸ”¹ Set Up MySQL RDS
Open AWS Console â†’ RDS
Click Create Database â†’ Select MySQL
Choose instance type (e.g., db.t2.micro)
Set username & password â†’ Save the endpoint URL
ğŸ“œ Logging
The ETL process logs events in logs/etl.log:

log
Copy
Edit
2025-02-27 10:00:00 - INFO - ETL Pipeline Started
2025-02-27 10:00:05 - INFO - Extracted data from file1.csv
2025-02-27 10:00:10 - INFO - Transformed data successfully
2025-02-27 10:00:15 - INFO - Loaded data into MySQL RDS
ğŸ“Œ To-Do List
 Extract data from different file formats
 Upload/download data from AWS S3
 Transform and clean data
 Load transformed data into AWS MySQL RDS
 Automate with AWS Glue (Optional)
 Schedule jobs with AWS Lambda (Optional)
